{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Laying the Foundation for Analysis\n",
        "\n",
        "## Objective\n",
        "Define the data analysis workflow and develop a thorough understanding of the model and data.\n",
        "\n",
        "This notebook implements Task 1 requirements:\n",
        "1. **Data Analysis Workflow**: Documented steps from data loading to insight generation\n",
        "2. **Event Data Research**: Compiled dataset of major oil market events\n",
        "3. **Assumptions and Limitations**: Documented with emphasis on correlation vs. causation\n",
        "4. **Communication Channels**: Identified formats for stakeholder communication\n",
        "5. **Time Series Properties**: Analysis of trend, stationarity, and volatility\n",
        "6. **Change Point Models**: Explanation of purpose and expected outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Analysis Workflow\n",
        "\n",
        "### Step 1: Data Loading and Validation\n",
        "- Load Brent crude oil price time series data (daily frequency recommended)\n",
        "- Validate data quality: check for missing values, outliers, and data integrity\n",
        "- Document data source, date range, and frequency\n",
        "- Create initial time series visualization to identify visual patterns\n",
        "\n",
        "### Step 2: Exploratory Data Analysis (EDA)\n",
        "- **Trend Analysis**: Decompose time series to identify long-term trends, test for deterministic vs stochastic trends\n",
        "- **Stationarity Testing**: Apply ADF, KPSS, and PP tests to determine if differencing is required\n",
        "- **Volatility Analysis**: Calculate rolling volatility, test for volatility clustering (ARCH effects), identify volatility regimes\n",
        "\n",
        "### Step 3: Event Data Integration\n",
        "- Load compiled event dataset\n",
        "- Align event dates with price data timeline\n",
        "- Categorize events by type (OPEC decisions, geopolitical events, economic shocks)\n",
        "- Visually assess price movements around event dates\n",
        "\n",
        "### Step 4: Change Point Detection\n",
        "- Select appropriate change point detection method (Bayesian change point models, CUSUM, Chow test)\n",
        "- Specify model parameters (mean shifts, variance changes, trend breaks)\n",
        "- Fit models and estimate change point locations with uncertainty intervals\n",
        "- Perform model diagnostics (convergence checks, model fit assessment)\n",
        "\n",
        "### Step 5: Results Interpretation and Validation\n",
        "- Extract most probable change point dates with credible intervals\n",
        "- Estimate regime-specific parameters (mean, variance, trend)\n",
        "- Compare detected change points with historical events\n",
        "- Assess statistical significance of detected breaks\n",
        "- Characterize each identified regime\n",
        "\n",
        "### Step 6: Visualization and Reporting\n",
        "- Create time series plots with overlaid change points\n",
        "- Visualize posterior distributions for change point uncertainty\n",
        "- Generate event timeline showing events and detected breaks\n",
        "- Produce summary statistics comparing regimes\n",
        "- Create executive summary and technical report\n",
        "\n",
        "### Step 7: Insight Generation\n",
        "- Identify patterns in structural breaks\n",
        "- Discuss potential causal relationships (with appropriate caveats)\n",
        "- Provide predictive insights for future price behavior\n",
        "- Assess risk periods of high uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "\n",
        "# Import project modules\n",
        "from data_loader import load_oil_price_data, validate_data, preprocess_data, load_event_data\n",
        "from eda import (\n",
        "    descriptive_statistics, \n",
        "    trend_analysis, \n",
        "    test_stationarity, \n",
        "    volatility_analysis,\n",
        "    autocorrelation_analysis\n",
        ")\n",
        "from event_integration import (\n",
        "    align_events_with_prices,\n",
        "    categorize_events,\n",
        "    calculate_event_impact_statistics\n",
        ")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Event Data Research and Compilation\n",
        "\n",
        "We have compiled a structured dataset containing **22 major oil market events** (2001-2023) covering:\n",
        "- OPEC production decisions\n",
        "- Geopolitical events (wars, conflicts, sanctions)\n",
        "- Economic shocks (financial crises, pandemics)\n",
        "- Natural disasters\n",
        "- Market events\n",
        "\n",
        "Let's load and examine the event data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load event data\n",
        "event_data_path = '../data/raw/oil_market_events.csv'\n",
        "events_df = load_event_data(event_data_path)\n",
        "\n",
        "print(f\"Total events compiled: {len(events_df)}\")\n",
        "print(f\"\\nEvent date range: {events_df['event_date'].min()} to {events_df['event_date'].max()}\")\n",
        "print(f\"\\nEvent types:\")\n",
        "print(events_df['event_type'].value_counts())\n",
        "print(f\"\\nImpact types:\")\n",
        "print(events_df['impact_type'].value_counts())\n",
        "print(f\"\\nSeverity distribution:\")\n",
        "print(events_df['severity'].value_counts())\n",
        "\n",
        "# Display the complete event table\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPLETE EVENT TABLE\")\n",
        "print(\"=\"*100)\n",
        "display(events_df.sort_values('event_date'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Validation\n",
        "\n",
        "Now let's load actual Brent crude oil price data and perform validation. We'll use yfinance to fetch historical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Brent crude oil price data using yfinance\n",
        "try:\n",
        "    import yfinance as yf\n",
        "    \n",
        "    # Fetch Brent crude oil futures (BZ=F) or use alternative ticker\n",
        "    # Note: yfinance may not have direct Brent ticker, so we'll use a proxy or load from file\n",
        "    print(\"Attempting to load Brent crude oil price data...\")\n",
        "    \n",
        "    # Option 1: Try to fetch from yfinance (if available)\n",
        "    # For demonstration, we'll create sample data structure\n",
        "    # In practice, load from your data source\n",
        "    \n",
        "    # Create date range for analysis\n",
        "    start_date = '2000-01-01'\n",
        "    end_date = '2024-12-31'\n",
        "    \n",
        "    print(f\"Data range: {start_date} to {end_date}\")\n",
        "    print(\"\\nNOTE: To load actual data, either:\")\n",
        "    print(\"1. Provide a CSV file with Brent prices in data/raw/\")\n",
        "    print(\"2. Use yfinance with appropriate ticker\")\n",
        "    print(\"3. Use FRED API for DCOILBRENTEU (Brent Crude Oil Price)\")\n",
        "    \n",
        "    # For demonstration, we'll show the data loading structure\n",
        "    # Replace this with actual data loading when available\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"yfinance not installed. Install with: pip install yfinance\")\n",
        "    print(\"Alternatively, load data from CSV file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Load from FRED API or create sample data for demonstration\n",
        "# This cell demonstrates the data loading workflow with actual implementation\n",
        "\n",
        "def load_brent_data_from_fred():\n",
        "    \"\"\"Load Brent crude oil price from FRED API.\"\"\"\n",
        "    try:\n",
        "        import pandas_datareader.data as web\n",
        "        from datetime import datetime\n",
        "        \n",
        "        # FRED series ID for Brent Crude Oil Price\n",
        "        series_id = 'DCOILBRENTEU'\n",
        "        start = datetime(2000, 1, 1)\n",
        "        end = datetime(2024, 12, 31)\n",
        "        \n",
        "        df = web.DataReader(series_id, 'fred', start, end)\n",
        "        df.columns = ['price']\n",
        "        df = df.dropna()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"FRED API loading failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_brent_data_from_yfinance():\n",
        "    \"\"\"Load Brent crude oil price from yfinance.\"\"\"\n",
        "    try:\n",
        "        import yfinance as yf\n",
        "        # Use a proxy ticker or ETF that tracks Brent\n",
        "        # CL=F is WTI, BZ=F might be Brent futures\n",
        "        ticker = yf.Ticker(\"BZ=F\")\n",
        "        df = ticker.history(start=\"2000-01-01\", end=\"2024-12-31\")\n",
        "        if not df.empty:\n",
        "            df = df[['Close']].rename(columns={'Close': 'price'})\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        print(f\"yfinance loading failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Try to load data\n",
        "print(\"Attempting to load Brent crude oil price data...\")\n",
        "price_df = None\n",
        "\n",
        "# Try FRED first\n",
        "price_df = load_brent_data_from_fred()\n",
        "if price_df is not None:\n",
        "    print(f\"✓ Loaded {len(price_df)} observations from FRED\")\n",
        "    print(f\"Date range: {price_df.index.min()} to {price_df.index.max()}\")\n",
        "else:\n",
        "    # Try yfinance\n",
        "    price_df = load_brent_data_from_yfinance()\n",
        "    if price_df is not None:\n",
        "        print(f\"✓ Loaded {len(price_df)} observations from yfinance\")\n",
        "        print(f\"Date range: {price_df.index.min()} to {price_df.index.max()}\")\n",
        "    else:\n",
        "        print(\"⚠ Could not load data from online sources.\")\n",
        "        print(\"Please provide a CSV file with Brent prices or install required packages:\")\n",
        "        print(\"  pip install pandas-datareader yfinance\")\n",
        "        print(\"\\nFor demonstration, creating sample data structure...\")\n",
        "        # Create sample date range\n",
        "        dates = pd.date_range(start='2000-01-01', end='2024-12-31', freq='D')\n",
        "        # Create placeholder (replace with actual data loading)\n",
        "        price_df = pd.DataFrame(index=dates, columns=['price'])\n",
        "        price_df['price'] = np.nan\n",
        "        print(\"Sample structure created. Replace with actual data loading.\")\n",
        "\n",
        "if price_df is not None and not price_df.empty:\n",
        "    # Validate data\n",
        "    validation_report = validate_data(price_df)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATA VALIDATION REPORT\")\n",
        "    print(\"=\"*80)\n",
        "    for key, value in validation_report.items():\n",
        "        if key != 'outliers':\n",
        "            print(f\"{key}: {value}\")\n",
        "    \n",
        "    # Display first few rows\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(price_df.head())\n",
        "    print(\"\\nLast 5 rows:\")\n",
        "    print(price_df.tail())\n",
        "    print(f\"\\nData shape: {price_df.shape}\")\n",
        "    print(f\"Missing values: {price_df.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Time Series Properties Analysis\n",
        "\n",
        "Now we perform comprehensive analysis of time series properties: **Trend Analysis**, **Stationarity Testing**, and **Volatility Analysis**. These analyses inform our modeling choices for change point detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Trend Analysis\n",
        "\n",
        "**Purpose**: Identify long-term directional movements in oil prices to understand if the series has deterministic trends, stochastic trends, or trend breaks.\n",
        "\n",
        "**Methods Applied**:\n",
        "- Moving averages (30-day and 60-day windows)\n",
        "- Linear trend fitting\n",
        "- Time series decomposition (trend, seasonal, residual components)\n",
        "\n",
        "**Modeling Implications**:\n",
        "- If **trend-stationary**: Include deterministic trend in change point model\n",
        "- If **difference-stationary**: Use differencing or include stochastic trend\n",
        "- If **trend breaks exist**: Model requires change point detection in trend component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Trend Analysis\n",
        "if price_df is not None and not price_df['price'].isna().all():\n",
        "    # Preprocess: handle missing values\n",
        "    price_df_clean = preprocess_data(price_df, handle_missing='forward_fill')\n",
        "    \n",
        "    # Perform trend analysis\n",
        "    print(\"=\"*80)\n",
        "    print(\"TREND ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    trend_results = trend_analysis(price_df_clean, window=30)\n",
        "    \n",
        "    # Extract results for price column\n",
        "    price_col = price_df_clean.columns[0]\n",
        "    \n",
        "    # Display linear trend results\n",
        "    if f\"{price_col}_linear_trend\" in trend_results:\n",
        "        linear_trend = trend_results[f\"{price_col}_linear_trend\"]\n",
        "        print(f\"\\nLinear Trend Analysis for {price_col}:\")\n",
        "        print(f\"  Slope: {linear_trend['slope']:.4f} (price units per day)\")\n",
        "        print(f\"  Intercept: {linear_trend['intercept']:.2f}\")\n",
        "        print(f\"  R-squared: {linear_trend['r_squared']:.4f}\")\n",
        "        print(f\"  P-value: {linear_trend['p_value']:.2e}\")\n",
        "        \n",
        "        if linear_trend['p_value'] < 0.05:\n",
        "            print(\"  → Significant linear trend detected\")\n",
        "        else:\n",
        "            print(\"  → No significant linear trend\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "    \n",
        "    # Plot 1: Price with moving averages\n",
        "    axes[0].plot(price_df_clean.index, price_df_clean[price_col], \n",
        "                 label='Brent Crude Price', alpha=0.6, linewidth=1)\n",
        "    if f\"{price_col}_ma_30\" in trend_results:\n",
        "        ma_30 = trend_results[f\"{price_col}_ma_30\"]\n",
        "        axes[0].plot(price_df_clean.index, ma_30, \n",
        "                    label='30-day MA', linewidth=2)\n",
        "    if f\"{price_col}_ma_60\" in trend_results:\n",
        "        ma_60 = trend_results[f\"{price_col}_ma_60\"]\n",
        "        axes[0].plot(price_df_clean.index, ma_60, \n",
        "                    label='60-day MA', linewidth=2)\n",
        "    \n",
        "    axes[0].set_title('Brent Crude Oil Price with Moving Averages', \n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Date')\n",
        "    axes[0].set_ylabel('Price (USD/barrel)')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Decomposed trend (if available)\n",
        "    if f\"{price_col}_trend\" in trend_results:\n",
        "        trend_component = trend_results[f\"{price_col}_trend\"]\n",
        "        axes[1].plot(price_df_clean.index, price_df_clean[price_col], \n",
        "                    label='Original', alpha=0.3)\n",
        "        axes[1].plot(trend_component.index, trend_component.values, \n",
        "                    label='Trend Component', linewidth=2, color='red')\n",
        "        axes[1].set_title('Time Series Decomposition - Trend Component', \n",
        "                         fontsize=14, fontweight='bold')\n",
        "        axes[1].set_xlabel('Date')\n",
        "        axes[1].set_ylabel('Price (USD/barrel)')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, 'Decomposition not available\\n(insufficient data or period)', \n",
        "                    ha='center', va='center', transform=axes[1].transAxes)\n",
        "        axes[1].set_title('Time Series Decomposition - Trend Component', \n",
        "                         fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../reports/figures/task1_trend_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n✓ Trend analysis completed\")\n",
        "else:\n",
        "    print(\"⚠ Cannot perform trend analysis: No price data available\")\n",
        "    print(\"Please load price data first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Stationarity Testing\n",
        "\n",
        "**Purpose**: Determine if statistical properties (mean, variance) are constant over time. This is crucial for change point detection because non-stationary data may require differencing or different modeling approaches.\n",
        "\n",
        "**Tests Applied**:\n",
        "- **Augmented Dickey-Fuller (ADF) Test**: Tests null hypothesis of unit root (non-stationary)\n",
        "- **Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test**: Tests null hypothesis of stationarity\n",
        "\n",
        "**Modeling Implications**:\n",
        "- If **non-stationary**: May require differencing or cointegration analysis before change point detection\n",
        "- If **stationary**: Can be modeled directly with change point models\n",
        "- **Structural breaks** can cause apparent non-stationarity, which is what we're trying to detect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Stationarity Testing\n",
        "if price_df is not None and not price_df['price'].isna().all():\n",
        "    price_df_clean = preprocess_data(price_df, handle_missing='forward_fill')\n",
        "    price_col = price_df_clean.columns[0]\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"STATIONARITY TESTING\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Test original series\n",
        "    print(f\"\\nTesting stationarity of {price_col} (original series):\")\n",
        "    stationarity_results = test_stationarity(price_df_clean[price_col], alpha=0.05)\n",
        "    \n",
        "    # Display ADF test results\n",
        "    if stationarity_results['adf']:\n",
        "        adf = stationarity_results['adf']\n",
        "        print(f\"\\nAugmented Dickey-Fuller (ADF) Test:\")\n",
        "        print(f\"  Test Statistic: {adf['test_statistic']:.4f}\")\n",
        "        print(f\"  P-value: {adf['p_value']:.4f}\")\n",
        "        print(f\"  Critical Values:\")\n",
        "        for level, value in adf['critical_values'].items():\n",
        "            print(f\"    {level}: {value:.4f}\")\n",
        "        print(f\"  Is Stationary: {adf['is_stationary']}\")\n",
        "        if adf['is_stationary']:\n",
        "            print(\"  → Series is STATIONARY (reject null hypothesis of unit root)\")\n",
        "        else:\n",
        "            print(\"  → Series is NON-STATIONARY (fail to reject null hypothesis)\")\n",
        "    \n",
        "    # Display KPSS test results\n",
        "    if stationarity_results['kpss']:\n",
        "        kpss = stationarity_results['kpss']\n",
        "        print(f\"\\nKwiatkowski-Phillips-Schmidt-Shin (KPSS) Test:\")\n",
        "        print(f\"  Test Statistic: {kpss['test_statistic']:.4f}\")\n",
        "        print(f\"  P-value: {kpss['p_value']:.4f}\")\n",
        "        print(f\"  Critical Values:\")\n",
        "        for level, value in kpss['critical_values'].items():\n",
        "            print(f\"    {level}: {value:.4f}\")\n",
        "        print(f\"  Is Stationary: {kpss['is_stationary']}\")\n",
        "        if kpss['is_stationary']:\n",
        "            print(\"  → Series is STATIONARY (fail to reject null hypothesis)\")\n",
        "        else:\n",
        "            print(\"  → Series is NON-STATIONARY (reject null hypothesis)\")\n",
        "    \n",
        "    # Combined conclusion\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CONCLUSION: {stationarity_results['conclusion']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Test first difference if original is non-stationary\n",
        "    if stationarity_results['conclusion'] in ['Non-stationary', 'Inconclusive - conflicting results']:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Testing First Difference (to check if series is difference-stationary):\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        price_diff = price_df_clean[price_col].diff().dropna()\n",
        "        diff_results = test_stationarity(price_diff, alpha=0.05)\n",
        "        \n",
        "        if diff_results['adf']:\n",
        "            adf_diff = diff_results['adf']\n",
        "            print(f\"\\nADF Test on First Difference:\")\n",
        "            print(f\"  Test Statistic: {adf_diff['test_statistic']:.4f}\")\n",
        "            print(f\"  P-value: {adf_diff['p_value']:.4f}\")\n",
        "            print(f\"  Is Stationary: {adf_diff['is_stationary']}\")\n",
        "            if adf_diff['is_stationary']:\n",
        "                print(\"  → First difference is STATIONARY\")\n",
        "                print(\"  → Original series is DIFFERENCE-STATIONARY (I(1))\")\n",
        "                print(\"  → Modeling implication: Use differenced series or include stochastic trend\")\n",
        "        \n",
        "        # Visualize original vs differenced\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "        \n",
        "        axes[0].plot(price_df_clean.index, price_df_clean[price_col])\n",
        "        axes[0].set_title('Original Series (Brent Crude Price)', fontsize=12, fontweight='bold')\n",
        "        axes[0].set_ylabel('Price (USD/barrel)')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1].plot(price_diff.index, price_diff.values)\n",
        "        axes[1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "        axes[1].set_title('First Difference', fontsize=12, fontweight='bold')\n",
        "        axes[1].set_xlabel('Date')\n",
        "        axes[1].set_ylabel('Price Change')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('../reports/figures/task1_stationarity_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    print(\"\\n✓ Stationarity testing completed\")\n",
        "else:\n",
        "    print(\"⚠ Cannot perform stationarity testing: No price data available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Volatility Analysis\n",
        "\n",
        "**Purpose**: Understand how price variability changes over time. Volatility clustering and regime changes are important for change point detection.\n",
        "\n",
        "**Methods Applied**:\n",
        "- Rolling volatility (30-day window)\n",
        "- Annualized volatility calculations\n",
        "- ARCH effects test (Ljung-Box test on squared returns) to detect volatility clustering\n",
        "\n",
        "**Modeling Implications**:\n",
        "- If **constant volatility**: Simple variance parameter in change point model\n",
        "- If **time-varying volatility**: Include volatility change points in model\n",
        "- If **volatility clustering detected**: May require GARCH-type models or volatility regime detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Volatility Analysis\n",
        "if price_df is not None and not price_df['price'].isna().all():\n",
        "    price_df_clean = preprocess_data(price_df, handle_missing='forward_fill')\n",
        "    price_col = price_df_clean.columns[0]\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"VOLATILITY ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    volatility_results = volatility_analysis(price_df_clean, window=30)\n",
        "    \n",
        "    # Display volatility statistics\n",
        "    if f\"{price_col}_volatility_stats\" in volatility_results:\n",
        "        vol_stats = volatility_results[f\"{price_col}_volatility_stats\"]\n",
        "        print(f\"\\nVolatility Statistics for {price_col}:\")\n",
        "        print(f\"  Mean Daily Volatility: {vol_stats['mean_volatility']:.4f}\")\n",
        "        print(f\"  Annualized Volatility: {vol_stats['annualized_volatility']:.2%}\")\n",
        "        print(f\"  Maximum Rolling Volatility: {vol_stats['max_volatility']:.4f}\")\n",
        "        print(f\"  Minimum Rolling Volatility: {vol_stats['min_volatility']:.4f}\")\n",
        "        print(f\"  Volatility of Volatility: {vol_stats['volatility_of_volatility']:.4f}\")\n",
        "    \n",
        "    # ARCH effects test\n",
        "    if f\"{price_col}_arch_test\" in volatility_results and volatility_results[f\"{price_col}_arch_test\"]:\n",
        "        arch_test = volatility_results[f\"{price_col}_arch_test\"]\n",
        "        print(f\"\\nARCH Effects Test (Volatility Clustering):\")\n",
        "        print(f\"  Ljung-Box Statistic: {arch_test['ljung_box_statistic']:.4f}\")\n",
        "        print(f\"  P-value: {arch_test['p_value']:.4f}\")\n",
        "        print(f\"  Has ARCH Effects: {arch_test['has_arch_effects']}\")\n",
        "        if arch_test['has_arch_effects']:\n",
        "            print(\"  → Volatility clustering detected (ARCH effects present)\")\n",
        "            print(\"  → Modeling implication: Consider GARCH models or volatility change points\")\n",
        "        else:\n",
        "            print(\"  → No significant volatility clustering detected\")\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "    \n",
        "    # Plot 1: Price series\n",
        "    axes[0].plot(price_df_clean.index, price_df_clean[price_col], linewidth=1, alpha=0.7)\n",
        "    axes[0].set_title('Brent Crude Oil Price', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_ylabel('Price (USD/barrel)')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Returns\n",
        "    returns = price_df_clean[price_col].pct_change().dropna()\n",
        "    axes[1].plot(returns.index, returns.values, linewidth=0.5, alpha=0.7, color='green')\n",
        "    axes[1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "    axes[1].set_title('Daily Returns', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_ylabel('Return')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Rolling volatility\n",
        "    if f\"{price_col}_rolling_volatility\" in volatility_results:\n",
        "        rolling_vol = volatility_results[f\"{price_col}_rolling_volatility\"]\n",
        "        axes[2].plot(rolling_vol.index, rolling_vol.values, linewidth=2, color='red', label='30-day Rolling Volatility')\n",
        "        if f\"{price_col}_rolling_volatility_annualized\" in volatility_results:\n",
        "            rolling_vol_ann = volatility_results[f\"{price_col}_rolling_volatility_annualized\"]\n",
        "            ax2_twin = axes[2].twinx()\n",
        "            ax2_twin.plot(rolling_vol_ann.index, rolling_vol_ann.values, \n",
        "                         linewidth=2, color='orange', alpha=0.7, \n",
        "                         label='Annualized Volatility')\n",
        "            ax2_twin.set_ylabel('Annualized Volatility', color='orange')\n",
        "            ax2_twin.tick_params(axis='y', labelcolor='orange')\n",
        "        axes[2].set_title('Rolling Volatility (30-day window)', fontsize=12, fontweight='bold')\n",
        "        axes[2].set_xlabel('Date')\n",
        "        axes[2].set_ylabel('Daily Volatility', color='red')\n",
        "        axes[2].tick_params(axis='y', labelcolor='red')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        axes[2].legend(loc='upper left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../reports/figures/task1_volatility_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Volatility regime identification\n",
        "    if f\"{price_col}_rolling_volatility\" in volatility_results:\n",
        "        rolling_vol = volatility_results[f\"{price_col}_rolling_volatility\"]\n",
        "        vol_mean = rolling_vol.mean()\n",
        "        vol_std = rolling_vol.std()\n",
        "        \n",
        "        high_vol_threshold = vol_mean + vol_std\n",
        "        low_vol_threshold = vol_mean - vol_std\n",
        "        \n",
        "        high_vol_periods = (rolling_vol > high_vol_threshold).sum()\n",
        "        low_vol_periods = (rolling_vol < low_vol_threshold).sum()\n",
        "        \n",
        "        print(f\"\\nVolatility Regime Analysis:\")\n",
        "        print(f\"  Mean Volatility: {vol_mean:.4f}\")\n",
        "        print(f\"  Std of Volatility: {vol_std:.4f}\")\n",
        "        print(f\"  High Volatility Periods (> mean + 1 std): {high_vol_periods} days\")\n",
        "        print(f\"  Low Volatility Periods (< mean - 1 std): {low_vol_periods} days\")\n",
        "        print(f\"  → Suggests time-varying volatility, supporting need for volatility change points\")\n",
        "    \n",
        "    print(\"\\n✓ Volatility analysis completed\")\n",
        "else:\n",
        "    print(\"⚠ Cannot perform volatility analysis: No price data available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Change Point Models: Explanation and Purpose\n",
        "\n",
        "### 5.1 What are Change Point Models?\n",
        "\n",
        "Change point models are statistical methods designed to identify **structural breaks** in time series data - points where the underlying data-generating process changes. In the context of oil prices, these breaks represent fundamental shifts in market dynamics.\n",
        "\n",
        "### 5.2 Purpose in Oil Price Analysis\n",
        "\n",
        "Change point models help us identify when and where structural breaks occur in Brent crude oil prices. These breaks may be caused by:\n",
        "\n",
        "1. **Supply Shocks**: \n",
        "   - OPEC production decisions (cuts/increases)\n",
        "   - Pipeline disruptions\n",
        "   - Geopolitical conflicts affecting production\n",
        "   - Natural disasters\n",
        "\n",
        "2. **Demand Shocks**:\n",
        "   - Economic recessions (reduced demand)\n",
        "   - Economic booms (increased demand)\n",
        "   - Technological changes (e.g., electric vehicles)\n",
        "   - Policy shifts (e.g., carbon taxes)\n",
        "\n",
        "3. **Market Structure Changes**:\n",
        "   - Financialization of oil markets\n",
        "   - Regulatory changes\n",
        "   - New trading mechanisms\n",
        "\n",
        "4. **Regime Shifts**:\n",
        "   - Transition from one market equilibrium to another\n",
        "   - Long-term structural changes in supply/demand balance\n",
        "\n",
        "### 5.3 How Change Point Models Work\n",
        "\n",
        "Change point models identify breaks by:\n",
        "\n",
        "1. **Statistical Detection**: Objectively identify break points without requiring prior knowledge of when events occurred\n",
        "2. **Uncertainty Quantification**: Provide probability distributions for break locations (not just point estimates)\n",
        "3. **Multiple Breaks**: Can detect multiple structural breaks simultaneously\n",
        "4. **Parameter Estimation**: Estimate regime-specific parameters (mean, variance, trend) for each identified period\n",
        "\n",
        "### 5.4 Types of Change Points\n",
        "\n",
        "1. **Mean Shifts**: Sudden changes in the average price level\n",
        "2. **Variance Changes**: Changes in volatility/uncertainty\n",
        "3. **Trend Breaks**: Changes in the direction or slope of trends\n",
        "4. **Combined Changes**: Simultaneous changes in multiple parameters\n",
        "\n",
        "### 5.5 Expected Outputs\n",
        "\n",
        "When we run change point analysis, we expect to get:\n",
        "\n",
        "1. **Change Point Dates**:\n",
        "   - Most probable break dates\n",
        "   - Uncertainty intervals (credible intervals showing range of possible break dates)\n",
        "   - Posterior probability distributions\n",
        "\n",
        "2. **Regime Parameters**:\n",
        "   - Mean price levels for each regime\n",
        "   - Volatility (variance) for each regime\n",
        "   - Trend parameters (if applicable)\n",
        "   - Transition probabilities (if using regime-switching models)\n",
        "\n",
        "3. **Model Diagnostics**:\n",
        "   - Convergence statistics (for Bayesian methods)\n",
        "   - Model fit metrics\n",
        "   - Posterior predictive checks\n",
        "\n",
        "### 5.6 Limitations and Caveats\n",
        "\n",
        "**Critical Limitation - Correlation vs. Causation**:\n",
        "- Detected breaks may **correlate** with events but don't **prove causation**\n",
        "- Multiple events may occur simultaneously\n",
        "- Markets may anticipate events (prices change before official dates)\n",
        "- Additional causal inference methods needed to establish causation\n",
        "\n",
        "**Other Limitations**:\n",
        "- Results depend on model assumptions and specifications\n",
        "- Different models may yield different break points\n",
        "- Temporal resolution limited by data frequency\n",
        "- Testing for multiple breaks increases false positive risk\n",
        "- Detected breaks are historical and may not be predictive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Assumptions and Limitations\n",
        "\n",
        "### Key Assumptions\n",
        "\n",
        "#### Data Assumptions\n",
        "- **Data Quality**: Brent crude price data is accurate, complete, and free from systematic errors\n",
        "- **Data Frequency**: Daily frequency is appropriate for detecting structural breaks\n",
        "- **Market Representation**: Brent crude prices are representative of global oil market dynamics\n",
        "- **Data Availability**: Sufficient historical data is available to identify meaningful patterns\n",
        "\n",
        "#### Model Assumptions\n",
        "- **Change Point Model**: Structural breaks can be adequately modeled using chosen change point detection methods\n",
        "- **Parameter Stability**: Within each regime, parameters (mean, variance) are relatively stable\n",
        "- **Independence**: Errors are independent across regimes (may not hold for financial time series)\n",
        "- **Linearity**: Linear relationships within regimes (may not capture non-linear dynamics)\n",
        "- **Prior Distributions**: In Bayesian models, prior distributions reasonably reflect prior knowledge\n",
        "\n",
        "#### Event Data Assumptions\n",
        "- **Event Dates**: Event dates accurately represent the true timing of market impact\n",
        "- **Event Impact**: Events have immediate or near-immediate impact on prices (may not account for anticipation effects)\n",
        "- **Event Completeness**: The compiled event list captures all major market-moving events\n",
        "- **Event Classification**: Events can be meaningfully categorized (supply/demand/geopolitical)\n",
        "\n",
        "#### Statistical Assumptions\n",
        "- **Stationarity**: Within regimes, data is stationary or can be made stationary\n",
        "- **Distribution**: Data follows specified distribution (e.g., normal, t-distribution)\n",
        "- **Homoscedasticity**: Within regimes, constant variance (may not hold)\n",
        "- **Sample Size**: Sufficient observations within each regime for reliable parameter estimation\n",
        "\n",
        "### Critical Limitations\n",
        "\n",
        "#### ⚠️ Correlation vs. Causation - MOST IMPORTANT LIMITATION\n",
        "\n",
        "**This is the most critical limitation**: The analysis can identify **statistical correlations** between structural breaks and events, but **cannot prove causation**.\n",
        "\n",
        "**Why this matters**:\n",
        "- **Temporal Correlation ≠ Causation**: Just because a structural break occurs near an event date does not mean the event caused the break\n",
        "- **Confounding Factors**: Multiple events may occur simultaneously, making it difficult to attribute breaks to specific causes\n",
        "- **Anticipation Effects**: Markets may react before events occur (e.g., prices may change in anticipation of OPEC decisions)\n",
        "- **Reverse Causation**: Price changes may influence events (e.g., high prices may trigger policy responses)\n",
        "- **Omitted Variables**: Unobserved factors may drive both events and price changes\n",
        "\n",
        "**What this means**:\n",
        "- Detected breaks may be **associated** with events but not necessarily **caused** by them\n",
        "- Additional evidence (e.g., event studies, causal inference methods) would be needed to establish causation\n",
        "- Results should be interpreted as **suggestive** rather than **definitive**\n",
        "\n",
        "#### Other Limitations\n",
        "\n",
        "**Methodological Limitations**:\n",
        "- **Model Specification**: Results depend on chosen model structure (number of change points, parameterization)\n",
        "- **Model Uncertainty**: Different models may yield different break points\n",
        "- **Overfitting Risk**: Complex models may detect spurious breaks\n",
        "- **Multiple Testing**: Testing for multiple breaks increases false positive rates\n",
        "- **Temporal Resolution**: Limited by data frequency (cannot detect breaks within a day if using daily data)\n",
        "\n",
        "**Data Limitations**:\n",
        "- **Missing Data**: Missing observations may affect break detection\n",
        "- **Data Revisions**: Historical data may be revised, affecting results\n",
        "- **Single Market**: Analysis focuses on Brent crude; other benchmarks may show different patterns\n",
        "- **Time Period**: Results are limited to the available historical period\n",
        "\n",
        "**Interpretation Limitations**:\n",
        "- **Backward-Looking**: Analysis is historical; may not predict future breaks\n",
        "- **Context-Dependent**: Results may not generalize to different time periods or markets\n",
        "- **Event Attribution**: Difficult to attribute breaks to specific events without additional evidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Communication Channels\n",
        "\n",
        "### Primary Formats for Stakeholder Communication\n",
        "\n",
        "1. **Executive Summary** (1-2 pages)\n",
        "   - High-level overview for decision-makers\n",
        "   - Focus on key change points, regime characteristics, and practical implications\n",
        "   - Visual timeline of events and breaks\n",
        "   - Risk assessment summary\n",
        "\n",
        "2. **Technical Report**\n",
        "   - Detailed methodology and model specifications\n",
        "   - Statistical results and diagnostics\n",
        "   - Model comparison and validation\n",
        "   - Full results tables and figures\n",
        "\n",
        "3. **Interactive Visualizations**\n",
        "   - Time series plots with overlaid change points\n",
        "   - Event timeline visualizations\n",
        "   - Regime comparison charts\n",
        "   - Interactive dashboards (Plotly, Streamlit)\n",
        "\n",
        "4. **Jupyter Notebooks**\n",
        "   - Reproducible analysis notebooks for technical audiences\n",
        "   - Step-by-step methodology\n",
        "   - Code and results together\n",
        "\n",
        "5. **Presentations**\n",
        "   - Slide decks for stakeholder meetings\n",
        "   - Key findings and visualizations\n",
        "   - Q&A preparation materials\n",
        "\n",
        "### Key Messages to Communicate\n",
        "\n",
        "- **Change Point Locations**: When structural breaks occurred (with uncertainty intervals)\n",
        "- **Regime Characteristics**: How different periods differ (mean, volatility, trends)\n",
        "- **Event Associations**: Which events correlate with detected breaks\n",
        "- **Uncertainty Quantification**: Confidence levels in detected breaks\n",
        "- **Practical Implications**: What this means for decision-making\n",
        "- **Limitations**: Clear statement that correlations do not prove causation\n",
        "\n",
        "### Stakeholder-Specific Considerations\n",
        "\n",
        "- **Executives**: Focus on actionable insights and risk periods\n",
        "- **Analysts**: Provide detailed methodology and statistical evidence\n",
        "- **Risk Managers**: Emphasize uncertainty and limitations\n",
        "- **Policy Makers**: Highlight correlation vs. causation distinction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Understanding the Model and Data\n",
        "\n",
        "### 5.1 Time Series Properties Analysis\n",
        "\n",
        "Before modeling, we need to investigate the Brent oil price data for key properties that will inform our modeling choices:\n",
        "\n",
        "#### Trend Analysis\n",
        "- **Purpose**: Identify long-term directional movements in oil prices\n",
        "- **Methods**: Linear/non-linear trend fitting, unit root testing, structural trend breaks\n",
        "- **Modeling Implications**: \n",
        "  - If trend-stationary: Include deterministic trend in model\n",
        "  - If difference-stationary: Use differencing or include stochastic trend\n",
        "  - If trend breaks exist: Model requires change point detection in trend component\n",
        "\n",
        "#### Stationarity Testing\n",
        "- **Purpose**: Determine if statistical properties (mean, variance) are constant over time\n",
        "- **Methods**: ADF, KPSS, PP tests\n",
        "- **Modeling Implications**:\n",
        "  - Non-stationary data may require differencing or cointegration analysis\n",
        "  - Stationary data can be modeled directly\n",
        "  - Structural breaks can cause apparent non-stationarity\n",
        "\n",
        "#### Volatility Patterns\n",
        "- **Purpose**: Understand how price variability changes over time\n",
        "- **Methods**: Rolling standard deviation, GARCH models, volatility clustering tests\n",
        "- **Modeling Implications**:\n",
        "  - Constant volatility: Simple variance parameter\n",
        "  - Time-varying volatility: Include volatility change points\n",
        "  - Volatility clustering: May require GARCH-type models\n",
        "\n",
        "### 5.2 Change Point Models\n",
        "\n",
        "#### Purpose in Oil Price Analysis\n",
        "Change point models identify structural breaks where the underlying data-generating process changes. In oil prices, these breaks may occur due to:\n",
        "- **Supply Shocks**: OPEC production cuts/increases, pipeline disruptions, geopolitical conflicts\n",
        "- **Demand Shocks**: Economic recessions, technological changes, policy shifts\n",
        "- **Market Structure Changes**: Financialization of oil markets, regulatory changes\n",
        "- **Regime Shifts**: Transition from one equilibrium to another\n",
        "\n",
        "#### How They Help Identify Structural Breaks\n",
        "- **Statistical Detection**: Objectively identify break points without prior knowledge\n",
        "- **Uncertainty Quantification**: Provide probability distributions for break locations\n",
        "- **Multiple Breaks**: Can detect multiple structural breaks simultaneously\n",
        "- **Parameter Estimation**: Estimate regime-specific parameters (mean, variance, trend)\n",
        "\n",
        "### 5.3 Expected Outputs\n",
        "\n",
        "#### Change Point Analysis Outputs\n",
        "1. **Change Point Dates**: \n",
        "   - Most probable break dates\n",
        "   - Uncertainty intervals (credible intervals)\n",
        "   - Posterior probability distributions\n",
        "\n",
        "2. **Regime Parameters**:\n",
        "   - Mean price levels for each regime\n",
        "   - Volatility (variance) for each regime\n",
        "   - Trend parameters (if applicable)\n",
        "   - Transition probabilities (if using regime-switching models)\n",
        "\n",
        "3. **Model Diagnostics**:\n",
        "   - Convergence statistics\n",
        "   - Model fit metrics\n",
        "   - Posterior predictive checks\n",
        "\n",
        "#### Limitations of Change Point Analysis\n",
        "- **Correlation vs Causation**: Detected breaks may correlate with events but don't prove causation\n",
        "- **Model Uncertainty**: Results depend on model assumptions and specifications\n",
        "- **Temporal Resolution**: Limited by data frequency (daily vs monthly)\n",
        "- **Multiple Hypotheses**: Testing for multiple breaks increases false positive risk\n",
        "- **Post-Hoc Analysis**: Detected breaks may not be predictive\n",
        "- **Event Attribution**: Difficult to attribute breaks to specific events without additional evidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Loading and Initial Exploration\n",
        "\n",
        "Now let's load the Brent crude oil price data and perform initial validation. For this demonstration, we'll show the workflow. In practice, you would load actual price data from your source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Data loading workflow\n",
        "# In practice, replace this with actual data loading\n",
        "# For demonstration, we'll create a note about data requirements\n",
        "\n",
        "print(\"DATA LOADING WORKFLOW\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "To load Brent crude oil price data:\n",
        "\n",
        "1. Option 1: Load from CSV file\n",
        "   price_df = load_oil_price_data(\n",
        "       file_path='data/raw/brent_prices.csv',\n",
        "       start_date='2000-01-01',\n",
        "       end_date='2024-12-31',\n",
        "       frequency='D'\n",
        "   )\n",
        "\n",
        "2. Option 2: Fetch from online source (e.g., FRED, Yahoo Finance)\n",
        "   - Implement API calls in data_loader.py\n",
        "   - Use libraries like yfinance, pandas_datareader, or FRED API\n",
        "\n",
        "3. Validate data quality\n",
        "   validation_report = validate_data(price_df)\n",
        "   print(validation_report)\n",
        "\n",
        "4. Preprocess if needed\n",
        "   price_df_clean = preprocess_data(price_df, handle_missing='forward_fill')\n",
        "\"\"\")\n",
        "\n",
        "# For demonstration, we'll note that actual data loading would happen here\n",
        "print(\"\\nNOTE: Actual price data should be loaded here for full analysis.\")\n",
        "print(\"The workflow functions are ready in src/data_loader.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Exploratory Data Analysis (EDA) Workflow\n",
        "\n",
        "Once data is loaded, we perform comprehensive EDA to understand time series properties. The following code demonstrates the workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA Workflow Example\n",
        "# This demonstrates the workflow - replace with actual data when available\n",
        "\n",
        "print(\"EXPLORATORY DATA ANALYSIS WORKFLOW\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "When price data is loaded, perform the following analyses:\n",
        "\n",
        "# 1. Descriptive Statistics\n",
        "stats = descriptive_statistics(price_df)\n",
        "print(stats)\n",
        "\n",
        "# 2. Trend Analysis\n",
        "trend_results = trend_analysis(price_df, window=30)\n",
        "# Access results:\n",
        "# - trend_results['price_ma_30']: 30-day moving average\n",
        "# - trend_results['price_linear_trend']: Linear trend parameters\n",
        "# - trend_results['price_trend']: Decomposed trend component\n",
        "\n",
        "# 3. Stationarity Testing\n",
        "for col in price_df.select_dtypes(include=[np.number]).columns:\n",
        "    stationarity_results = test_stationarity(price_df[col])\n",
        "    print(f\"\\n{col} Stationarity Test Results:\")\n",
        "    print(f\"ADF Test: {stationarity_results['adf']}\")\n",
        "    print(f\"KPSS Test: {stationarity_results['kpss']}\")\n",
        "    print(f\"Conclusion: {stationarity_results['conclusion']}\")\n",
        "\n",
        "# 4. Volatility Analysis\n",
        "volatility_results = volatility_analysis(price_df, window=30)\n",
        "# Access results:\n",
        "# - volatility_results['price_rolling_volatility']: Rolling volatility\n",
        "# - volatility_results['price_volatility_stats']: Volatility statistics\n",
        "# - volatility_results['price_arch_test']: ARCH effects test\n",
        "\n",
        "# 5. Autocorrelation Analysis\n",
        "for col in price_df.select_dtypes(include=[np.number]).columns:\n",
        "    acf_vals, pacf_vals = autocorrelation_analysis(price_df[col], lags=40)\n",
        "    # Plot ACF and PACF to identify AR/MA components\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nNOTE: Run these analyses when actual price data is loaded.\")\n",
        "print(\"All functions are available in src/eda.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Event Data Integration Workflow\n",
        "\n",
        "Integrate the compiled event data with price data to assess correlations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Event Integration Workflow\n",
        "print(\"EVENT DATA INTEGRATION WORKFLOW\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "When price data is loaded, integrate events as follows:\n",
        "\n",
        "# 1. Align events with prices\n",
        "events_with_prices = align_events_with_prices(\n",
        "    price_df=price_df,\n",
        "    event_df=events_df,\n",
        "    price_column='price',  # Adjust to your column name\n",
        "    event_date_column='event_date',\n",
        "    window_days=30  # Analyze 30 days before/after each event\n",
        ")\n",
        "\n",
        "# 2. Categorize events\n",
        "event_categories = categorize_events(events_with_prices)\n",
        "print(\"Event categories:\", list(event_categories.keys()))\n",
        "\n",
        "# 3. Calculate impact statistics\n",
        "impact_stats = calculate_event_impact_statistics(events_with_prices)\n",
        "print(\"\\nEvent Impact Statistics:\")\n",
        "print(impact_stats)\n",
        "\n",
        "# 4. Visualize events on price timeline\n",
        "# (Create plots showing price movements around event dates)\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nEvent data is ready for integration when price data is loaded.\")\n",
        "print(\"All functions are available in src/event_integration.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary: Task 1 Deliverables\n",
        "\n",
        "### ✅ Completed Deliverables\n",
        "\n",
        "1. **Data Analysis Workflow** (Section 1)\n",
        "   - Documented 7-step workflow from data loading to insight generation\n",
        "   - Clear process for each analysis stage\n",
        "\n",
        "2. **Event Data Table** (Section 2)\n",
        "   - Compiled 22 major oil market events (2001-2023)\n",
        "   - Structured CSV file with dates, types, descriptions, impact types, and severity\n",
        "   - Exceeds minimum requirement of 10-15 events\n",
        "\n",
        "3. **Assumptions and Limitations** (Section 3)\n",
        "   - Comprehensive documentation of all assumptions\n",
        "   - **Critical discussion on correlation vs. causation** - the most important limitation\n",
        "   - Methodological, data, and interpretation limitations\n",
        "\n",
        "4. **Communication Channels** (Section 4)\n",
        "   - Identified 5 primary formats for stakeholder communication\n",
        "   - Key messages to communicate\n",
        "   - Stakeholder-specific considerations\n",
        "\n",
        "5. **Understanding Model and Data** (Section 5)\n",
        "   - Time series properties analysis framework (trend, stationarity, volatility)\n",
        "   - Change point model explanation and purpose\n",
        "   - Expected outputs and limitations\n",
        "\n",
        "6. **Implementation Code**\n",
        "   - Data loading functions (`src/data_loader.py`)\n",
        "   - EDA functions (`src/eda.py`)\n",
        "   - Event integration functions (`src/event_integration.py`)\n",
        "   - Workflow demonstrated in this notebook\n",
        "\n",
        "### Next Steps (Task 2+)\n",
        "\n",
        "- Load actual Brent crude oil price data\n",
        "- Perform full EDA analysis\n",
        "- Implement change point detection models\n",
        "- Compare detected breaks with event dates\n",
        "- Generate visualizations and reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Event Data Table Summary\n",
        "\n",
        "Below is a summary visualization of the compiled events:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary visualizations of event data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. Events by type\n",
        "event_counts = events_df['event_type'].value_counts()\n",
        "axes[0, 0].bar(event_counts.index, event_counts.values, color='steelblue')\n",
        "axes[0, 0].set_title('Number of Events by Type', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Event Type')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. Events by impact type\n",
        "impact_counts = events_df['impact_type'].value_counts()\n",
        "axes[0, 1].bar(impact_counts.index, impact_counts.values, color='coral')\n",
        "axes[0, 1].set_title('Number of Events by Impact Type', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Impact Type')\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Events by severity\n",
        "severity_counts = events_df['severity'].value_counts()\n",
        "axes[1, 0].bar(severity_counts.index, severity_counts.values, color='mediumseagreen')\n",
        "axes[1, 0].set_title('Number of Events by Severity', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Severity')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "\n",
        "# 4. Timeline of events\n",
        "events_df_sorted = events_df.sort_values('event_date')\n",
        "axes[1, 1].scatter(events_df_sorted['event_date'], \n",
        "                   range(len(events_df_sorted)), \n",
        "                   s=100, alpha=0.6, c='red')\n",
        "axes[1, 1].set_title('Timeline of Events (2001-2023)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Event Date')\n",
        "axes[1, 1].set_ylabel('Event Index')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/figures/task1_event_summary.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nEvent Summary Statistics:\")\n",
        "print(f\"Total events: {len(events_df)}\")\n",
        "print(f\"Date range: {events_df['event_date'].min()} to {events_df['event_date'].max()}\")\n",
        "print(f\"\\nEvents by type:\\n{event_counts}\")\n",
        "print(f\"\\nEvents by impact:\\n{impact_counts}\")\n",
        "print(f\"\\nEvents by severity:\\n{severity_counts}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
